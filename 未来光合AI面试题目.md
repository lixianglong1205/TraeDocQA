# 原始需求


```shell
分析下面的需求, 理清要实现的功能点:


# 基于 LangChain 构建一个智能数据问答 Agent
## 任务说明
假设公司有一份产品 FAQ 文档（提供给候选人一份 3-5 页的 PDF / TXT 文件），你需要在 2 小时内 使用 LangChain 构建一个 AI Agent，能够回答用户关于该产品的自然语言问题。

## 要求
### 数据处理

使用 LangChain 将 FAQ 文档进行分块并向量化存储（可选用 FAISS 或 Chroma）。

### Agent 设计

使用 LangChain 的 Agent 工具链（Tool + LLMChain）实现一个智能问答 Agent。

当用户提出问题时，Agent 应优先基于 FAQ 知识库回答。

如果知识库中没有相关信息，则返回「未找到相关内容」。

### 功能扩展

添加一个 计算器 Tool，允许用户在问答过程中进行简单计算，例如：

“这个产品比竞品贵 15%，如果竞品是 200 元，那价格是多少？”

### 接口 & 演示

提供一个简易的交互方式（命令行或 Streamlit 网页均可）。

用户输入问题后，Agent 输出回答。

### 加分项（可选）

使用 Memory 让 Agent 保持上下文对话能力。

允许用户上传新的 FAQ 文档并动态更新知识库。

### 交付内容
项目代码（最好结构清晰，如：app.py, utils/, requirements.txt）。

运行说明（如何安装依赖与启动 demo）。

简短的设计说明（100-200 字，介绍你实现思路和所用技术）。

### 评估标准
功能完整性（核心功能是否实现）

代码可读性与结构

对 LangChain 与 Agent 的合理使用

处理未知问题的鲁棒性

Bonus：扩展功能的实现
```

# 需求理解

```shell
# 基于 LangChain 构建一个智能数据问答 Agent
## 任务说明
假设公司有一份产品 FAQ 文档（提供给候选人一份 3-5 页的 PDF / TXT 文件），你需要在 2 小时内 使用 LangChain 构建一个 AI Agent，能够回答用户关于该产品的自然语言问题。

	基础功能: 文档智能问答, 基于lanchain开发的网页版/命令行方式的文档智能问答, 公司上传一个FAQ文档, 可以是PDF也可以是TXT文件, 系统自动解析文件, 分块并存储到向量数据库中, 然后供在线用户提问的时候, 优先检索FAQ问, 如果检索到了, 就基于检索到的信息进行回复. 如果没有检索到, 就回复未找到相关内容.

	拓展功能: 添加一个计算工具Tool, 将自然语言的计算, 写成FC, 调Tool进行计算, 并回答经过计算后的结果.

	加分项: 
		Memory功能, 让Agent保持上下文对话能力
			[执行思考] 滑动窗口上下文; embedding语义相关记忆检索, 取Top N, 不超过max_tokens限制即可; 结构化记忆提取与检索; 
		用户上传新的FAQ文档, 动态更新知识库
			[执行思考] 
	交付说明:
		结构清晰、README.md中带运行说明(依赖安装、启动命令)
		100-200字的设计说明, 实现思想和用到的技术

## 要求
### 数据处理

使用 LangChain 将 FAQ 文档进行分块并向量化存储（可选用 FAISS 或 Chroma）。

### Agent 设计

使用 LangChain 的 Agent 工具链（Tool + LLMChain）实现一个智能问答 Agent。

当用户提出问题时，Agent 应优先基于 FAQ 知识库回答。

如果知识库中没有相关信息，则返回「未找到相关内容」。

### 功能扩展

添加一个 计算器 Tool，允许用户在问答过程中进行简单计算，例如：

“这个产品比竞品贵 15%，如果竞品是 200 元，那价格是多少？”

### 接口 & 演示

提供一个简易的交互方式（命令行或 Streamlit 网页均可）。

用户输入问题后，Agent 输出回答。

### 加分项（可选）

使用 Memory 让 Agent 保持上下文对话能力。

允许用户上传新的 FAQ 文档并动态更新知识库。

### 交付内容
项目代码（最好结构清晰，如：app.py, utils/, requirements.txt）。

运行说明（如何安装依赖与启动 demo）。

简短的设计说明（100-200 字，介绍你实现思路和所用技术）。

### 评估标准
功能完整性（核心功能是否实现）

代码可读性与结构

对 LangChain 与 Agent 的合理使用

处理未知问题的鲁棒性

Bonus：扩展功能的实现


	## 功能与实现草稿:
		### AI编程的硬性要求:
			必须用uv来管理和配置python环境依赖
			必须用langchain来开发Agent
			必须严格按照代码整洁之道命名文件、编写代码, 保持代码结构清晰并尽量没有冗余
			必须用同步代码逻辑编写, 不用多线程、多进程、异步等方式编写代码
			项目中用到的所有的 chat LLM 必须选择 deepseek-reasoner 类型的, 模型版本选择DeepSeek-V3.2
（思考模式）, 秘钥是: sk-ec35f0e7ee2f4cefb18a7a04041bcd1b
		### 网页开发: 
			网页开发用Streamlit框架
			上传文档页面;
				文档格式校验, 用正则表达式的方式, 检查上传的文件是否是PDF/TXT,如果不是则跳出警告.
			chat提问交互页面, 支持流式输出;
				仿照chatGPT能流式输出即可, 带上点赞、点踩、点重新生成等功能.
		### 文档解析
			PDF文件, 用langchain框架的PDF解析器. 目标, 对于纯文本转的PDF文件, 输入这个解析工具, 输出是文本.
			TXT文件, 用langchain框架的TXT解析器, 但是要考虑文件是windows还是mac/Linux编码格式的
			如果文件读取失败, 前端页面给出预警信息. 但是不终止整个系统
		### FAQ提取
			上一轮输入的是非结构化的数据, 需要找个算法转结构化. 用LLM来做(基座模型选择deepseek). 滑动窗口的方式, 2k tokens为窗口, 每次滑动窗口, 重复500个tokens, 抽取里面的'问题-回答'对. 输出是一个'问题-答案对'列表, Json格式, 示例输出: [{"问题": "***?", "答案": "***?"}, {"问题": "***?", "答案": "***?"}]
		### FAQ存向量数据库
			选择langchain的向量数据库, chroma, 用它来存储FAQ的向量数据
			选择一个embedding模型, 这里选择调第三方的embedding api. 需要找一个国产化的, 或者假设能搞到openai的秘钥, 直接调openai的接口获取embedding
				注意, 这里选择openai的embedding模型. 不再考虑国产化的embedding或者本地部署的embedding
		### 用户提问与系统响应逻辑和流程
			问题识别, 用LLM来判断是否是一个问句(二分类任务, 输出是或者否, 用规则解析输出内容来执行后续动作), 如果是, 走后续逻辑, 不是的话; 以聊天的方式, 和用户沟通交流.
			意图识别, 判断用户的提问是否是一个计算问题(二分类任务, 输出是或者否, 用规则解析输出内容来执行后续动作), 是的话, 调另外一个计算器Tool. 并根据计算器结果回复用户. 不是的话走下面的流程
			对于问句, 调embedding接口编码, 去库里语义检索, 然后召回N个相关的问题, 用LLM判断这几个FAQ问题是否和用户的提问问题相关, 相关的话走后续流程, 不相关的话, 输出「未找到相关内容」
			用LLM判断哪个检索到的FAQ是和用户提问的问题相关的(二分类任务, 对每个检索到的FAQ都判断一遍, 输出是或者否, 用规则解析输出内容来执行后续动作), 将相关的FAQ作为召回的内容, 拼接prompt回复用户的提问
```



# PRD文档

# 智能文档问答系统产品需求文档（PRD）

## 1. 概述
本系统旨在构建一个基于文档的智能问答应用，支持用户上传文档（PDF/TXT），自动提取“问题-答案”对（FAQ），并通过语义检索与大型语言模型（LLM）驱动的方式，为用户提供精准、流畅的问答交互体验。

## 2. 技术架构与约束

### 2.1 开发环境与框架
- **Python 环境管理**：必须使用 `uv` 进行依赖管理与环境配置。
- **Agent 开发框架**：必须使用 `LangChain` 进行 Agent 的开发与集成。
- **代码规范**：必须遵循《代码整洁之道》原则进行命名与编写，确保代码结构清晰、无冗余。
- **并发模型**：必须使用同步代码逻辑编写，禁止使用多线程、多进程或异步方式。
- **前端框架**：使用 `Streamlit` 进行网页开发。

### 2.2 模型选型
- **对话 LLM**：必须使用 `deepseek-reasoner` 类型模型，版本选择 `DeepSeek-V3.2`（思考模式）。
- **Embedding 模型**：选用 OpenAI 的 embedding API，不采用国产化或本地部署方案。

## 3. 功能模块详述

### 3.1 网页前端（Streamlit）
#### 3.1.1 文档上传页面
- 提供文件上传组件，支持 PDF 与 TXT 格式。
- 格式校验：通过正则表达式校验文件后缀，若非 PDF/TXT，前端弹出警告提示。
- 用户交互：上传后显示文件名称与状态。

#### 3.1.2 聊天交互页面
- 仿照 ChatGPT 交互风格，支持流式输出回答。
- 功能按钮：支持对回答进行“点赞”、“点踩”、“重新生成”操作。
- 对话历史：保留当前会话的问答记录。

### 3.2 文档解析模块
- **PDF 解析**：使用 LangChain 提供的 PDF 解析器，针对纯文本型 PDF 输出文本内容。
- **TXT 解析**：使用 LangChain 的文本解析器，兼容 Windows（如 GBK）与 Mac/Linux（如 UTF-8）编码格式。
- **容错处理**：若文件读取失败，前端显示预警信息，但不中断系统运行，用户可重新上传。

### 3.3 FAQ 提取模块
- **输入**：非结构化文本（来自文档解析）。
- **处理流程**：
  - 采用滑动窗口法，窗口大小为 2K tokens，滑动重叠 500 tokens。
  - 每个窗口内容送入 LLM（DeepSeek 基座模型）进行“问题-答案”对抽取。
- **输出格式**：JSON 列表，示例：
  ```json
  [
    {"问题": "示例问题1?", "答案": "示例答案1。"},
    {"问题": "示例问题2?", "答案": "示例答案2。"}
  ]
  ```

### 3.4 向量存储模块
- **向量数据库**：使用 LangChain 集成的 Chroma 向量数据库存储 FAQ 的嵌入向量。
- **Embedding 生成**：调用 OpenAI Embedding API 将 FAQ 中的“问题”文本转换为向量。
- **索引构建**：FAQ 存入后即构建向量索引，支持后续语义检索。

### 3.5 问答处理流程
1. **问题识别**  
   - 使用 LLM 判断用户输入是否为提问句（二分类任务，输出“是”/“否”）。
   - 若非问句，系统以闲聊模式与用户交互。

2. **意图识别**  
   - 使用 LLM 判断提问是否为计算类问题（二分类任务）。
   - 若是计算问题，调用计算器 Tool 处理并返回结果。

3. **语义检索与相关性筛选**  
   - 用户提问经 Embedding 接口编码后，在向量库中检索 Top-N 个相似 FAQ 问题。
   - 使用 LLM 判断检索到的 FAQ 是否与用户提问相关：
     - 若均不相关，回复“未找到相关内容”。
     - 若相关，进入答案生成阶段。

4. **答案生成**  
   - 对每个检索到的 FAQ，使用 LLM 进行二分类判断是否与用户提问匹配。
   - 将匹配的 FAQ 答案与问题一起拼接为 Prompt，送入 LLM 生成最终回复。

## 4. 非功能性需求
- **可用性**：前端交互简洁，流式输出延迟低于 2 秒。
- **可维护性**：代码结构清晰，模块解耦，便于后续迭代。
- **安全性**：API 密钥等敏感信息通过环境变量管理，不入库。

## 5. 输出与交付物
- 可运行的 Streamlit 应用。
- 完整的代码仓库，包含清晰目录结构与 README。
- 配置说明文档，包括环境安装、依赖部署与启动步骤。

---
**备注**：本文档为系统实现的基础蓝图，具体实现中需保持模块化开发与充分测试，确保各环节稳定衔接。





---

<end>
